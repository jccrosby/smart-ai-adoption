---
#theme: seriph
# random image from a curated Unsplash collection by Anthony
# like them? see https://unsplash.com/collections/94734566/slidev
background: https://cover.sli.dev
# apply unocss classes to the current slide
# class: text-center
# https://sli.dev/features/drawing
drawings:
  persist: false
# slide transition: https://sli.dev/guide/animations.html#slide-transitions
transition: slide-left
# enable MDC Syntax: https://sli.dev/features/mdc
mdc: true
---

# Smart AI Adoption: How to Evaluate AI for Your Team (10 min)

- AI beyond engineering: AI’s role in **product, design, and development**
- The **AI Evaluation Rubric**: Questions every team should ask:
  - Does this AI tool solve a real problem, or is it just “cool”?
  - How does it impact efficiency vs. quality?
  - What are the risks (bias, security, ethics)?
  - Can we trust the outputs?
  - How does it fit into our existing workflows?
- **Red flags to watch for** in AI adoption.

---
layout: evaluation-framework
---

## AI Adoption Rubric

The AI Evaluation Rubric comprises several critical questions that teams should ask before adopting any AI tool. These questions aim to ensure a strategic and problem-centric approach to AI adoption, moving beyond mere trend-following. According to the sources, the key questions include:

1. **Does this AI tool solve a real problem, or is it just ‘cool’?** - This question emphasizes the need for practical application and value-driven adoption, ensuring that AI implementation addresses genuine needs rather than being driven by novelty.
2. **How does it impact efficiency vs. quality?** - This prompts a consideration of the trade-offs involved in using AI, encouraging teams to analyze whether the gains in speed or cost come at the expense of the quality of their work.
3. **What are the risks (bias, security, ethics)?** - This highlights the importance of responsible AI implementation, urging teams to think critically about potential negative consequences related to bias in algorithms, security vulnerabilities in AI systems, and the ethical implications of using the tool.
4. **Can we trust the outputs?** - This addresses concerns about the reliability and accuracy of AI-generated content, emphasizing the need to evaluate the trustworthiness of AI outputs before relying on them in critical processes.
5. **How does it fit into our existing workflows?** - This stresses the need for seamless integration of AI tools into current team processes, rather than disruptive implementation that could hinder productivity.

By addressing these questions, teams can make more informed decisions about AI adoption, ensuring that it serves as a valuable tool for augmentation and incremental improvement rather than a source of unforeseen problems or inefficiencies. The rubric helps to operationalize the "learn, evaluate, apply" framework for smart AI adoption.
