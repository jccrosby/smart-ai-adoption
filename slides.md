---
# You can also start simply with 'default'
theme: ./theme
# some information about your slides (markdown enabled)
title: AI for Teams - Smart Adoption Not Blind FOMO
info: |
  AI won’t take your job—but someone using it smarter will. Don’t fall for the hype or the FOMO. This talk gives you the tools to evaluate AI, adopt it wisely, and build a practical rubric for making smart AI decisions—for you, your career, and your team.
author: John Crosby
# apply unocss classes to the current slide
class: text-center
# https://sli.dev/features/drawing
drawings:
  persist: false
fonts:
  sans: Lora
  serif: Roboto Slab
  mono: Fira Code
# slide transition: https://sli.dev/guide/animations.html#slide-transitions
transition: slide-left
exportFilename: smart-ai-adoption
# enable MDC Syntax: https://sli.dev/features/mdc
mdc: false
colorSchema: auto
---

# AI for Teams - Smart Adoption Not Blind FOMO


---
layout: intro
introImage: /images/john-crosby.jpg
bsky: jccrosby.com
---

# John Crosby

## Principal Engineer, Client Engineering @ MLB

<!--
- In a previous life I was a Professional Chef.
- I transitioned from cooking to project managing for a 3D animation and web development company.
- That's where I caught the programming bug and haven't really looked back since.
- Then started a web development and training consultancy and helped to run that for just over a decade.
- Then I joined MLB and have been working in the MLB.tv and streaming space.
- This is my 11th season at MLB.
- That said, I believe I have a non-traditional background for all of this, but it's allowed me to deal with change and uncertainty in a different way.
-->

---
layout: new-section
sectionImage: /images/smart-vs-fomo-chatgpt.png
imageWidth: 512
imageHeight: 341
---

# Navigating AI for Teams

## Smart Adoption vs. Blind FOMO

<!--
1. AI for Teams - Smart Adoption Not Blind FOMO
2. AI is going to take all the jobs!
3. There is a lot of fear mongering out there
4. "AI won’t take your job—but someone using it smarter will.""
5. ...the pressure to "do something with AI" is real.
6. But the reality is that AI is a tool, not a magic wand.
7. The key is to understand it and adopt it wisely, not blindly.
8. That's my hope for this talk - to help you navigate the AI landscape, avoid the pitfalls, and make informed decisions.
-->

---
layout: image
image: /images/ai-landscape.png
---

<div class="bg-[#fff]/70 p-10 rounded-lg text-red-500 text-7xl text-align-center">The Current Landscape</div>

<!--
1. Questions:
   1. Who here leads a team?
   2. How about multiple teams?
   3. How about the entire group or company?
   4. How many of you have said or heard some form of "we need to do something with AI"?
2. We're experiencing rapid change due to AI advancements
   1. Especially around LLMs and generative AI.
3. There are already lots of experiments using AI tools in many different ways.
4. People and teams may feel overwhelmed by the hype and the sheer volume of advancements, news, and available tools.
-->

---
layout: default
---

# The Dual Edge: Hype vs. Opportunity

## The pressure to "do something with AI" can lead to rushed decisions & poor outcomes

<!--
1. Opportunity AI offers benefits like:
   1. Enhanced productivity
   2. Better quality quality
   3. Faster cycles
   4. Potential for innovation
2. The risk is jumping in without a plan
3. AI project failure rates remain high due to flawed or non-existent strategies when it comes to adoption
4. Common Pitfalls: Lack of clear goals, no coherent strategy, neglecting data readiness, and "Blind FOMO"
5. The challenge is to navigate this landscape wisely, so you can harness the potential of AI
-->

---
layout: default
---

# Smart Adoption vs. Blind FOMO: The Core Difference

## How AI is adopted is more critical than whether it is adopted.

<!--
1. Smart Adoption:
   1. Involves learning and planning before any significant investment
   2. That means
      1. Being deliberate
      2. Being strategic
      3. Focusing on solving specific problems
      4. Creating measurable results so you know when you've succeeded (or not)
      5. Assessing the results and iterating
2. Blind FOMO
   1. Often skips learning and planning
   2. Reactive
   3. Technology-centric
   4. Driven by hype rather than a specific problem
   5. Doesn't measure and you can't clearly assess the results
3. How AI is adopted is more critical than whether it is adopted
-->

---
layout: new-section
sectionImage: /images/core-principles.jpeg
---

# Core Principles of Effective AI Adoption

## Foundational Principals to Smart Adoption

---
layout: default
---

# Six Core Principles

1. **Strategic:** Define _why_ before _what_.
2. **Data Readiness & Governance:** Poor data leads to failure.
3. **Skills & Culture:** Learning, experimentation, critical assessment, and sharing are key.
4. **Technology & Infrastructure:** The right tools in the right place.
5. **Ethics & Responsibility:** Fairness, transparency, privacy, security
6. **Iterative Approach:** Short feedback loops and continuous learning.

<!--
1. Strategic: AI initiatives must solve specific problems aligned with business goals.
   1. You need to know the _why_ before _what_.
2. Data Readiness & Governance: Strong data quality, accessibility, security, and governance are essential.
   1. Poor data leads to failure.
   2. Garbage in, garbage out.
3. Skills & Culture: Requires technical skills and a supportive culture encouraging experimentation, learning, and sharing.
   1. Learning, experimentation, critical assessment, and sharing are key to the success of.
   2. Enabling this kind of culture can only increase the success of your AI efforts.
4. Technology & Infrastructure: Select the right tools compatible with existing workflows.
   1. The right tools in the right place.
5. Ethics & Responsibility: IntMake sure ethic ethical considerations are in place from the beginning.
   1. Fairness, transparency, privacy, security
   2. Be willing to update and adjust where it makes sense.
6. Iterative Approach: Start with small pilots, learn, then scale those successful experiments.
   1. Short feedback loops and continuous learning.

Keeping these principles in mind will help you navigate the AI landscape and make informed decisions.
-->


---
layout: new-section
sectionImage: /images/LEAP-cycle.png
---

# The L.E.A.P. Framework

## An Iterative Process for Smart Adoption


<!--
1. Keeping the six core principles in mind, lead me to the LEAP framework:
   1. It is focused on maximizing value while mitigating risks.
   2. It encourages iterative progress and critical evaluation.
   3. It emphasizes measurement and learning from _successes_ AND _failures_.
   4. It provides a sustainable and flexible process that can
      1. Be applied at any level of an organization
      2. Adapted to fit specific needs.

In short, It's a structured repeatable way to help navigate AI adoption.

Let's break down the specific steps.
-->

---
layout: two-cols-header
---

# 1. LEARN

## Understand AI capabilities and limitations

::left::

# Activities

- Research AI tools and advancements.
- Identify pain points in your process.
- Assess team skills.
- Evaluate data availability and quality.
- Consider ethical and security implications.
- Define **SMART** goals for experiments.

::right::

# Output

- Prioritized use cases.
- Readiness assessment.
- Defined success metrics.

<!--
- Number 1 - Learn: Understand AI capabilities and limitations
- This is the foundation for the rest of the LEAP process.

Activities:

 1. Research AI tools and advancements
 2. Identify pain points in your process
 3. Assess team skills
 4. Evaluate data availability and quality
 5. Consider ethical and security implications
 6. Define SMART goals for experiments

Output:

1. Prioritized use cases
2. Readiness assessment
3. Defined success metrics

- The goal is to identify specific areas where AI can add value.
- This is where you're
  - Identifying the right use cases
  - Understanding the landscape
  - Creating your plan.
-->

---
layout: two-cols-header
---

# 2. EXPERIMENT

## Focus on small-scale, controlled pilots.

::left::

# Activities

- Select a low-risk pilot project.
- Choose specific AI tool/model.
- Develop and execute a prototype or limited integration.
- Gather quantitative and qualitative feedback.

::right::

# Output

- Working prototype.
- Performance data.
- User feedback.
- Lessons learned.

<!--
- Number 2 - Experiment: Focus on small-scale, controlled pilots to test you're assumptions and hypotheses.
- This is where you put your plan into action.

Activities:

1. Select a low-risk pilot project
2. Choose specific AI tool/model
3. Develop and execute a prototype or limited integration
4. Gather quantitative and qualitative feedback

Output:

1. Working prototype
2. Performance data
3. User feedback
4. Lessons learned

- The goal is to validate your assumptions and gather data on the AI's performance.
- This is where you can start to see the potential of AI in your workflow.
-->

---
layout: two-cols-header
---

# 3. ASSESS

## Critically evaluate outcomes to make better decisions.

::left::

# Activities

- Evaluate results against your goals.
- Analyze the benefits vs. costs
- Assess impact on workflow, quality, productivity, satisfaction.
- Identify challenges and risks.
- Review ethical and security concerns.
- Make a clear **Go/No-Go/Pivot** decision.

::right::

# Output

- A more refined understanding of requirements and the results.
- An assessment report.
- A clear decision.

<!--
- Number 3 - Assess: Evaluation experiment outcomes rigorously to make informed decisions.
- This is where you take a step back and look critically at the results of your experiment.

Activities:

1. Evaluate results against your goals
2. Analyze benefits vs. costs (ROI)
3. Assess impact on the things that matter (workflow, quality, productivity, satisfaction)
4. Identify challenges and risks
5. Review ethical and security concerns.
6. Make a clear **Go/No-Go/Pivot** decision

Output:

1. A more refined understanding of requirements and the results.
2. An assessment report
3. A Clear decision

- The goal is to determine if the AI is adding value and if it is worth scaling or not.
-->

---
layout: default
---

<h1 class="absolute left-4 pl-30">PIVOT</h1>
<h1 class="absolute right-6 pr-30">PROPAGATE</h1>

<!--
- Based on the **Go/No-Go/Pivot** decision from the asses step, you have a couple options:
  - Pivot
  - Propagate
-->
---
layout: two-cols-header
---

# 4. PIVOT

## If the experiment fails or needs adjustment

::left::

# Activities
- Analyze failure points and gather feedback.
- Adjust goals, tools, or methods based on insights.
- Reassess readiness and risks.
- Iterate on the experiment with a new focus or move to another experiment.

::right::

# Output
- Revised experiment plan.
- Updated goals.
- A new approach to testing.

<!--
- It didn't work how you expected or it failed outright. That's good. We get to learn from failure.
- Number 4 - Pivot: You're coming up with a whole new experiment or you're adjusting the current experiment.

Activities:

1. Analyze failure points and gather feedback.
2. Adjust goals, tools, or methods based on insights.
3. Reassess readiness and risks.
4. Iterate on the experiment with a new focus or move to another experiment.

Output:

1. Revised experiment plan or brand new experiment.
2. Updated goals.

You're using what you've learned to create a more refined and better informed next experiment.
and you basically return to Step 1 - Learn and work through the process again.
-->

---
layout: two-cols-header
---

# PROPAGATE

## Strategically scale successful AI applications.

::left::

# Activities
  - Develop a detailed rollout roadmap.
  - Identify infrastructure and tooling changes.
  - Create and execute training programs.
  - Establish governance policies and monitoring.

::right::

# Output
- A useable AI solution.
- Maintenance & governance structures.
- Ongoing monitoring plan.
- Up-skilled team.

<!--
- On the other side of things, it was a success!
- Number 4 - Propagate: Strategically scale successful AI applications.
- Now you get to scale the successful experiment across your team and/or organization.

Activities:

1. Create roadmap to rollout the successful experiment. Which should include:`
   1. Infrastructure and tooling changes
   2. Training programs
   3. Governance policies and monitoring processes

Output:

1. A useable AI solution
2. Maintenance & governance structures
3. Ongoing monitoring plan
4. An up-skilled team/organization

- The goal is to ensure that the AI solution is sustainable and effective.

**Regardless of the decision to Pivot or Propagate, don't forget to share your results and lessons learned. Speaking of...**
-->

---
layout: statement
---

# **Sharing** is an essential part of expanding the benefits of any effort like this.

<!--
- I think sharing what you're doing and how you're doing it and what you've learned is key to any process like this.
- Sharing can happen at any stage of the process.
- The only rule is that you share.
-->

---
layout: new-section
sectionImage: /images/exec-fomo.png
---

# Analogy - Blind FOMO

## Cursor: AI Tooling For Coding

<!--
- This is a little contrived to protect the innocent, but it has roots based in a reality.
- From Management: Basically - "Use Cursor to be more productive!" (Increase velocity)
  1. Cursor is a generative AI tool for code - "The AI Code Editor - Built to make you extraordinarily productive, Cursor is the best way to code with AI."
  2. The team was told to "check it out".
  3. There was no clear goal or understanding of what anyone was supposed to do or how.
  4. InfoSec team hadn't approved Cursor at any level
  5. Legal team hadn't approved it at any level
  6. There was no training or onboarding.
  7. This might have also been presented on March 5th.
     1. Spring Training started on February 20th
     2. Opening Day was March 27th.
     3. So basically during one of the busiest times of the year for everyone
  8. For the stores sake, let's say it was presented to 30 engineers across multiple teams and concerns.
  9. So there was basically 30 individual "experiments" going on at once
     1. No clear goals
     2. No way to know what should be measured or how to measure it
     3. No timeline for when to expect results
  10. So it turned into a subjective and unbounded "some liked it, some didn't"
-->

---
layout: default
---

# The Cursor "Experiment"

- **Skipped Learn:** Vague goal, focused on tech over specific problems, and ignored readiness
- **Skipped Experiment/Assess:** Underestimated verification needs, overlooked accuracy and security risks, ignored domain/context limitations.
- **Failed Pivot/Propagate:** Resulted in chaos, inconsistent adoption, decreased productivity, potential security issues, no way to measure impact

<!--
When we try to apply LEAP to the Cursor "Experiment"

- **Skipped Learn:**
  - Vague goal ("increase velocity")
  - Focused on tech (GenAI) over specific problems
  - Ignored readiness (knowledge, skills, codebase, IP)
- **Skipped Experiment/Assess:**
  - Underestimated verification needs
  - Overlooked accuracy and security risks
  - Ignored domain/context limitations
- **Failed Pivot/Propagate:**
  - Resulted in chaos
  - Inconsistent adoption
  - Decreased productivity
  - Potential security issues
  - No way to measure impact

The experiment failed to provide any real value. I'm sure some enjoyed checking out a new tool, but it was a waste of time and effort.
-->

---
layout: new-section
sectionImage: /images/five-questions-chatgpt.png
---

# Smart AI Adoption: How to Evaluate AI for Your Team

## Five Questions to Ask

<!--
- I've been thinking about how we might to evaluate tools in a consistent way and have come up with five questions to ask.
-->

---
layout: new-section
sectionImage: /images/Q1-Solve-real-problems-gemini.jpeg
---

# Question 1:

# Does it solve a **real problem**, or is it just "cool"?

<!--
Question 1 - Does it solve a **real problem**, or is it just "cool"?

- **Aligns with Strategic Alignment:** Ensure the AI addresses a specific bottleneck or pain point.
- **Red Flag:** Vague use cases with no clear, tangible problem.
- **LEAP - Learn:** Identify team bottlenecks and potential AI solutions.
-->

---
layout: new-section
sectionImage: /images/Q2-Efficiency-quality-matrix-chatgpt.png
---

# Question 2:
# How does it impact **efficiency vs. quality**?

<!--
Question 2 - How does it impact **efficiency vs. quality**?

**Quality vs. Efficiency:** Matrix to evaluate the trade-offs between efficiency and quality.

- **Weigh the trade-offs:** For instance, a focus on speed alone can hide quality costs.
- **Red Flag:** Overemphasis on speed alone without considering accuracy.
  - This reminds me of "Vibe coding"
- **LEAP - Assess:** Analyze the impact on workflow, process, quality, & accuracy holistically.
-->


---
layout: new-section
sectionImage: /images/Q3-Risks.png
---

# Question 3:

# What are the **risks**?

<!--
Question 3 - What are the **risks**?

- **Integrate Ethics & Responsibility:**
  - Consider potential biases
  - Security vulnerabilities
  - Ethical implications
- **Red Flags:**
  - Unverified or biased data
  - Weak security
  - Lack of ethical guidelines
- **LEAP - Learn & Assess:** Proactively consider risks and evaluate the ethical posture
-->


---
layout: new-section
sectionImage: /images/Q4-Verification.jpeg
---

# Question 4:

# Can we **trust the outputs**?

<!--
Question 4 - Can we **trust the outputs**?

- **Emphasize Verification:**
  - Understand AI's decision-making process
  - What are the mechanisms to verify its results.
- **Red Flags:**
  - Black-box models
  - No verification process
  - Limited testing.
- **LEAP - Experiment & Assess:** Gather data and feedback on the reliability of AI outputs.
-->

---
layout: new-section
sectionImage: /images/Q5-Workflows.png
---

# Question 5:

# How does it fit with our **existing workflows**?

<!--
Question 5 - How does it fit with out **existing workflows**?

- **Technology & Infrastructure:** Ensure compatibility and minimize disruption.
- **Red Flags:**
  - Major workflow disruption
  - Inadequate training
  - Lack of IT/operational readiness
- **Propagate:** Roll out the changes or pivot to a new experiment.
-->

---
layout: image-right
image: /images/red-flags-gemini.jpeg
---

## Red Flags to Watch For

- Hype-driven adoption
- Vague use cases
- Overemphasis on speed alone
- No clear KPIs
- Hidden quality costs
- Unverified data sources
- Weak security protocols
- No ethical guidelines
- Black-box models with no explanation
- Inadequate user training
- Lack of operational readiness
- No mechanism for verification
- Limited testing
- Major workflow disruption

<!--
Red Flags to Watch For

- Hype-driven adoption
- Vague use cases
- Overemphasis on speed alone
- No clear KPIs
- Hidden quality costs
- Unverified data sources
- Weak security protocols
- No ethical guidelines
- Black-box models with no explanation
- Inadequate user training
- Lack of operational readiness
- No mechanism for verification
- Limited testing
- Major workflow disruption
-->

---
layout: new-section
sectionImage: /images/Practical-AI-Use-Cases.png
---

# Practical AI Use Cases

## Leveraging AI Across Concerns

<!--
- Now let's look at some practical use cases for AI across different concerns.
- These are just a few examples of how I've seen AI used in different areas.
- I've attempted to align them with the LEAP framework.
-->

---
layout: default
---

# AI in Requirements Engineering

- NLP/LLMs for eliciting, analyzing, and validating requirements.
- Generating user stories and initial system models.
- **LEAP:**
  - **Learn:** Analyze requirements for ambiguity using NLP.
  - **Experiment:** Use NLP to analyze a small set of user stories.
  - **Assess:** Gather feedback on improved clarity.
  - **Propagate:** Scale use of validated NLP tools.

<!--
AI in Requirements Engineering

- Natural language processing and LLMs for eliciting, analyzing, and validating requirements.
- Generating user stories and initial system models.
- **LEAP:**
  - **Learn:** Analyze requirements for ambiguity using NLP.
  - **Experiment:** Use NLP to analyze a small set of user stories.
  - **Assess:** Gather feedback on improved clarity.
  - **Propagate:** Scale use of validated NLP tools.
-->

---
layout: default
---

# AI-Assisted Code

- Tools like GitHub Copilot, Cursor, Gemini Code Assist, Amazon Q, and Claude Code.
- Capabilities: code generation, explanation, refactoring suggestions, bug fixes and identification.
- **LEAP:**
  - **Learn:** Research tools and their security implications.
  - **Experiment:** Pilot with a small team on specific tasks.
  - **Assess:** Evaluate code quality and developer productivity.
  - **Propagate:** Roll out with training and best practices.

<!--
- Tools like GitHub Copilot, Cursor, Gemini Code Assist, Amazon Q, and Claude Code.
- Capabilities: code generation, explanation, refactoring suggestions, bug fixes and identification.
- **LEAP:**
  - **Learn:** Research tools and their security implications.
  - **Experiment:** Pilot with a small team on specific tasks.
  - **Assess:** Evaluate code quality and developer productivity.
  - **Propagate:** Roll out with training and best practices.
-->

---
layout: default
---

# AI-Driven QA

- LLMs and specialized models for generating unit, integration, and end-to-end tests.
- AI for GUI testing, fuzz testing, visual regression, bug detection, chaos testing.
- **LEAP:**
  - **Learn:** Identify repetitive testing tasks.
  - **Experiment:** Use AI to generate tests for non-critical components.
  - **Assess:** Measure code coverage and effort required.
  - **Propagate:** Integrate successful techniques into QA workflows.

<!--
AI-Assisted Code

- LLMs and specialized models for generating unit, integration, and end-to-end tests.
- AI for GUI testing, fuzz testing, visual regression, bug detection.
- **LEAP:**
  - **Learn:** Identify repetitive testing tasks.
  - **Experiment:** Use AI to generate tests for non-critical components.
  - **Assess:** Measure code coverage and effort required.
  - **Propagate:** Integrate successful techniques into QA workflows.
-->

---
layout: default
---

# AI in DevOps

- Automation of CI/CD, intelligent monitoring, anomaly detection, predictive failure analysis.
- MLOps platforms for managing ML pipelines.
- **LEAP:**
  - **Learn:** Identify manual DevOps bottlenecks.
  - **Experiment:** Automate a deployment or set of deployments.
  - **Assess:** Measure impact on deployment time and reliability.
  - **Propagate:** Expand automation across more services.

<!--
AI in DevOps

- Automation of CI/CD, intelligent monitoring, anomaly detection, predictive failure analysis.
- MLOps platforms for managing ML pipelines.
- **LEAP:**
  - **Learn:** Identify manual DevOps bottlenecks.
  - **Experiment:** Automate a deployment or set of deployments.
  - **Assess:** Measure impact on deployment time and reliability.
  - **Propagate:** Expand automation across more services.
-->

---
layout: default
---

# AI-Assisted Project Management and Reporting

- AI for task prioritization, resource allocation, and risk management.
- Automated reporting and status updates.
- **LEAP:**
  - **Learn:** Identify repetitive reporting tasks.
  - **Experiment:** Use AI to generate reports for a small project.
  - **Assess:** Measure time saved and accuracy.
  - **Propagate:** Integrate successful techniques into PM workflows.

<!--
AI-Assisted Project Management and Reporting

- Automation of CI/CD, intelligent monitoring, anomaly detection, predictive failure analysis.
- MLOps platforms for managing ML pipelines.
- **LEAP:**
  - **Learn:** Identify manual DevOps bottlenecks.
  - **Experiment:** Automate deployment of a microservice.
  - **Assess:** Measure impact on deployment time and reliability.
  - **Propagate:** Expand automation across more services.
-->

---
layout: default
---

# AI for Security Analysis

- Enhanced threat detection, log analysis, vulnerability assessment, automated Security Operations Center (SOC) tasks.
- AI tools for code vulnerability scanning.
- **LEAP:**
  - **Learn:** Research AI-powered security tools.
  - **Experiment:** Run Static Application Security Testing (SAST) tools on a feature branch.
  - **Assess:** Analyze false positive/negative rates.
  - **Propagate:** Integrate validated tools into the DevSecOps pipeline.

<!--
AI for Security Analysis

- Enhanced threat detection, log analysis, vulnerability assessment, automated Security Operations Center (SOC) tasks.
- AI tools for code vulnerability scanning.
- **LEAP:**
  - **Learn:** Research AI-powered security tools.
  - **Experiment:** Run Static Application Security Testing (SAST) tools on a feature branch.
  - **Assess:** Analyze false positive/negative rates.
  - **Propagate:** Integrate validated tools into the DevSecOps pipeline.
-->

---
layout: new-section
---

# Communicating AI Strategy to Leadership

## Emphasize Value, Risk Management, and the LEAP Framework

<!--
Communicating AI Strategy to Leadership

Emphasize Value, Risk Management, and the LEAP Framework

- Communicating AI strategy to leadership is crucial for gaining support and resources.
- Focus on the value AI can bring to the organization.
- Highlight the importance of risk management and ethical considerations.
- Use the LEAP framework to demonstrate a structured approach to AI adoption.
- Emphasize the iterative nature of the process and the importance of learning from both successes and failures.
-->

---
layout: cover
---

# Conclusion & Key Takeaways

## Embrace AI Thoughtfully and Strategically

<!--
Conclusion & Key Takeaways

Embrace AI Thoughtfully and Strategically

- AI is a powerful tool, but it requires careful consideration and planning.
- The LEAP framework provides a structured approach to AI adoption.
- Focus on learning, experimentation, assessment, and sharing.
-->

---
layout: default
---

# The Value of LEAP

- A simple, practical, repeatable process
- **Learn** before acting.
- **Experiment** in a controlled manner.
- **Assess** results critically.
- **Pivot** when necessary or **Propagate** successful initiatives sustainably.

## Don't forget to **share** your results and lessons learned.

<!--
The Value of LEAP

- It's a simple, practical, repeatable process
- **Learn** before acting.
- **Experiment** in a controlled manner.
- **Assess** critically.
- **Pivot** when necessary or **Propagate** successful initiatives sustainably.

**Don't forget to share your results and lessons learned.**
-->

---
layout: default
---

# Call to Action

- Move beyond passive observation of AI.
- **Initiate a LEAP cycle ->** start by identifying a pressing challenge.
- Research potential AI solutions.
- Define a clear goal for a small experiment.
- Embrace the learning process and iterate.
- Chart a course for smart AI adoption that delivers real value.

## And Again, **SHARE** your successes and failures!

<!--
Call to Action

- Move beyond passive observation of AI.
- **Start that LEAP cycle ->** identify a pressing challenge.
- Find potential AI solutions.
- Define a clear goal for a small experiment.
- Embrace the learning process and iterate.

Chart a course for smart AI adoption that delivers real value.

**And Again, SHARE your successes and failures!**
-->

---
layout: end
bsky: jccrosby.com
email: john@hellocrosby.com
github: https://github.com/jcrosby/smart-ai-adoption
---

<div class="text-8xl font-bold">Thank You!</div>

<!--
<div class="m-6 text-2xl">
  <a href="https://github.com/jcrosby/smart-ai-adoption" target="_blank" class="slidev-icon-btn">
    <carbon:logo-github />
  </a>
</div>
-->
